# Algorithmes de Clustering
Projet d'analyse et d'impl√©mentation d'algorithmes de classification non supervis√©e

## üìë Sommaire

1. [Contexte du projet](#contexte-du-projet)
2. [Veille technologique](#veille-technologique)
   - [Algorithmes de classification non supervis√©e](#algorithmes-de-classification-non-supervis√©e)
   - [M√©thodes de s√©lection du nombre optimal de clusters](#m√©thodes-de-s√©lection-du-nombre-optimal-de-clusters)
   - [Mesures de qualit√© d'un cluster](#mesures-de-qualit√©-dun-cluster)
3. [Impl√©mentation K-means et application sur Iris](#impl√©mentation-k-means-et-application-sur-iris)
   - [Classe K-means personnalis√©e](#classe-k-means-personnalis√©e)
   - [Application sur le dataset Iris](#application-sur-le-dataset-iris)
   - [Comparaison avec Scikit-Learn](#comparaison-avec-scikit-learn)
4. [Analyse du dataset Customer Personality](#analyse-du-dataset-customer-personality)
   - [Exploration et analyse des donn√©es](#exploration-et-analyse-des-donn√©es)
   - [Pr√©paration des donn√©es](#pr√©paration-des-donn√©es)
   - [R√©duction de dimension](#r√©duction-de-dimension)
5. [Application des algorithmes de clustering](#application-des-algorithmes-de-clustering)
   - [K-Means](#k-means)
   - [Hierarchical Clustering](#hierarchical-clustering)
   - [DBSCAN](#dbscan)
   - [Comparaison des algorithmes](#comparaison-des-algorithmes)
6. [√âvaluation et optimisation](#√©valuation-et-optimisation)
   - [D√©termination du nombre optimal de clusters](#d√©termination-du-nombre-optimal-de-clusters)
   - [M√©triques d'√©valuation](#m√©triques-d√©valuation)
7. [Profiling des groupes de clients](#profiling-des-groupes-de-clients)
8. [Conclusion](#conclusion)

---

## Contexte du projet

Ce projet vise √† √©tudier et appliquer les algorithmes de classification non supervis√©e (clustering) dans le cadre d'une analyse de donn√©es clients. Il se compose de deux parties principales :

1. **√âtude th√©orique et pratique des algorithmes de clustering** : Impl√©mentation d'une classe K-means personnalis√©e et application sur le dataset Iris
2. **Analyse compl√®te d'un dataset client** : Application de plusieurs algorithmes de clustering sur des donn√©es de marketing pour segmenter la client√®le

### Objectifs
- Comprendre le fonctionnement des algorithmes de clustering
- Impl√©menter l'algorithme K-means from scratch
- √âvaluer et comparer diff√©rents algorithmes de clustering
- Analyser les caract√©ristiques des groupes de clients identifi√©s

---

## Veille technologique

### Algorithmes de classification non supervis√©e

#### 1. K-Means
**Principe** : Algorithme de partitionnement qui divise les donn√©es en k clusters en minimisant la variance intra-cluster.

**Fonctionnement** :
- Initialisation al√©atoire de k centro√Ødes
- Assignation de chaque point au centro√Øde le plus proche
- Mise √† jour des centro√Ødes (moyenne des points assign√©s)
- R√©p√©tition jusqu'√† convergence

**Avantages** :
- Simple √† comprendre et impl√©menter
- Efficace sur de grands datasets
- Fonctionne bien avec des clusters sph√©riques

**Inconv√©nients** :
- N√©cessite de sp√©cifier k √† l'avance
- Sensible √† l'initialisation
- Assume des clusters de forme sph√©rique

#### 2. Hierarchical Clustering (Classification Hi√©rarchique)
**Principe** : Cr√©e une hi√©rarchie de clusters par fusion (agglom√©rative) ou division (divisive) successive.

**Fonctionnement** :
- Agglom√©ratif : Commence avec chaque point comme cluster, fusionne les plus proches
- Divisif : Commence avec un seul cluster, divise r√©cursivement
- Utilise une matrice de distances et un crit√®re de liaison

**Avantages** :
- Pas besoin de sp√©cifier le nombre de clusters √† l'avance
- Produit un dendrogramme informatif
- D√©terministe (pas d'al√©atoire)

**Inconv√©nients** :
- Complexit√© O(n¬≥) pour n points
- Sensible aux outliers
- Difficile √† modifier une fois construit

#### 3. DBSCAN (Density-Based Spatial Clustering)
**Principe** : Groupe les points qui sont √©troitement regroup√©s et marque les points isol√©s comme outliers.

**Fonctionnement** :
- Utilise deux param√®tres : eps (rayon) et min_samples (points minimum)
- Identifie les points core, border et noise
- Forme des clusters bas√©s sur la densit√©

**Avantages** :
- Peut trouver des clusters de forme arbitraire
- Robuste aux outliers
- D√©termine automatiquement le nombre de clusters

**Inconv√©nients** :
- Sensible aux param√®tres eps et min_samples
- Difficile avec des densit√©s variables
- Performance d√©grad√©e en haute dimension

### M√©thodes de s√©lection du nombre optimal de clusters

#### M√©thode du coude (Elbow Method)
**Principe** : Trace la variance intra-cluster en fonction du nombre de clusters et cherche le "coude" de la courbe.

**Mise en ≈ìuvre** :
- Calcule la somme des carr√©s intra-cluster (WCSS) pour diff√©rentes valeurs de k
- Trace WCSS vs k
- Identifie le point o√π la d√©croissance devient moins prononc√©e

**Limites** :
- Subjectif dans l'identification du coude
- Peut √™tre ambigu si plusieurs coudes existent

#### Silhouette Score
**Principe** : Mesure la qualit√© du clustering en comparant la coh√©sion intra-cluster et la s√©paration inter-cluster.

**Calcul** :
- Pour chaque point : s(i) = (b(i) - a(i)) / max(a(i), b(i))
- a(i) : distance moyenne aux points du m√™me cluster
- b(i) : distance moyenne au cluster le plus proche

**Interpr√©tation** :
- Score entre -1 et 1
- Plus proche de 1 = meilleur clustering
- N√©gatif = point mal class√©

#### Gap Statistic
**Principe** : Compare la variance intra-cluster observ√©e avec celle attendue sous une distribution uniforme.

**Avantages** :
- Approche statistique rigoureuse
- Moins subjective que la m√©thode du coude
- Fournit une estimation statistique du nombre optimal

**Mise en ≈ìuvre** :
- G√©n√®re des donn√©es de r√©f√©rence uniformes
- Compare la log(WCSS) observ√©e vs r√©f√©rence
- Choisit k o√π le gap est maximal

### Mesures de qualit√© d'un cluster

#### Coh√©sion intra-cluster
**Mesures de compacit√©** :
- Somme des carr√©s intra-cluster (WCSS)
- Distance moyenne des points au centro√Øde
- Variance intra-cluster

**Objectif** : Minimiser la dispersion des points au sein de chaque cluster

#### S√©paration inter-cluster
**Mesures de s√©paration** :
- Distance entre centro√Ødes
- Somme des carr√©s inter-cluster (BCSS)
- Distance minimum entre clusters

**Objectif** : Maximiser la distance entre les diff√©rents clusters

#### Indices de validit√©
**Indices internes** :
- Calinski-Harabasz Index : Rapport BCSS/WCSS
- Davies-Bouldin Index : Moyenne des ratios intra/inter-cluster
- Dunn Index : Ratio distance min inter/max intra

**Indices externes** (si v√©rit√© terrain disponible) :
- Adjusted Rand Index (ARI)
- Normalized Mutual Information (NMI)
- Homogeneity, Completeness, V-measure

---

## Impl√©mentation K-means et application sur Iris

### Classe K-means personnalis√©e
Impl√©mentation from scratch de l'algorithme K-means avec :
- Initialisation al√©atoire des centro√Ødes
- Assignation des points aux clusters
- Mise √† jour it√©rative des centro√Ødes
- Crit√®re de convergence

### Application sur le dataset Iris
**Objectifs** :
- Tester la classe K-means personnalis√©e
- Comparer avec l'impl√©mentation Scikit-Learn
- Analyser la stabilit√© sur plusieurs ex√©cutions
- √âvaluer pour diff√©rentes valeurs de k

**Observations attendues** :
- Variabilit√© des r√©sultats due √† l'initialisation al√©atoire
- Impact du nombre de clusters sur la qualit√©
- Diff√©rences potentielles avec Scikit-Learn

### Comparaison avec Scikit-Learn
Comparaison des performances, stabilit√© et r√©sultats entre :
- Impl√©mentation personnalis√©e
- sklearn.cluster.KMeans
- Analyse des diff√©rences et similitudes

---

## Analyse du dataset Customer Personality

### Exploration et analyse des donn√©es
**Dataset** : Marketing Campaign (Customer Personality Analysis)
- **Source** : Donn√©es de campagne marketing d'une √©picerie
- **Variables** : D√©mographiques, comportementales, transactionnelles
- **Objectif** : Segmentation client pour ciblage marketing

**Analyse exploratoire** :
- Statistiques descriptives
- Visualisations des distributions
- Identification des variables pertinentes
- D√©tection d'outliers et valeurs manquantes

### Pr√©paration des donn√©es
**√âtapes de preprocessing** :
- Nettoyage des donn√©es (valeurs manquantes, outliers)
- Encodage des variables cat√©gorielles
- Normalisation/standardisation des variables num√©riques
- Gestion des variables d√©riv√©es (√¢ge, anciennet√© client)

### R√©duction de dimension
**S√©lection de features** :
- Analyse de corr√©lation
- Importance des variables
- √âlimination des variables redondantes

**Analyse factorielle multiple (MFA)** :
- R√©duction de la dimensionnalit√©
- Pr√©servation de l'information pertinente
- Visualisation en 2D/3D

---

## Application des algorithmes de clustering

### K-Means
**Impl√©mentation** :
- Application de l'algorithme pour diff√©rentes valeurs de k
- Analyse de la stabilit√© sur plusieurs ex√©cutions
- Utilisation de k-means++ pour l'initialisation

**Param√®tres test√©s** :
- Nombre de clusters : 2 √† 10
- Diff√©rentes initialisations
- Crit√®res de convergence

### Hierarchical Clustering
**Impl√©mentation** :
- Clustering agglom√©ratif
- Diff√©rents crit√®res de liaison (ward, complete, average)
- Construction et analyse du dendrogramme

**Analyse** :
- Dendrogramme pour visualiser la hi√©rarchie
- Seuil de coupure pour d√©terminer le nombre de clusters
- Comparaison des crit√®res de liaison

### DBSCAN
**Impl√©mentation** :
- Recherche des param√®tres optimaux (eps, min_samples)
- Analyse des outliers d√©tect√©s
- √âvaluation de la robustesse

**Param√®tres** :
- eps : Distance maximale entre points voisins
- min_samples : Nombre minimum de points pour former un cluster
- Analyse de sensibilit√© aux param√®tres

### Comparaison des algorithmes
**Crit√®res de comparaison** :
- Qualit√© des clusters (silhouette, inertie)
- Stabilit√© des r√©sultats
- Temps d'ex√©cution
- Facilit√© d'interpr√©tation
- Robustesse aux outliers

---

## √âvaluation et optimisation

### D√©termination du nombre optimal de clusters
**M√©thodes appliqu√©es** :
- M√©thode du coude (Elbow Method)
- Silhouette Score
- Gap Statistic
- Calinski-Harabasz Index

**Analyse comparative** :
- Convergence des diff√©rentes m√©thodes
- Choix du nombre optimal de clusters
- Validation crois√©e des r√©sultats

### M√©triques d'√©valuation
**Scores calcul√©s** :
- Silhouette Score par cluster et global
- Inertie intra-cluster
- Indices de validit√© interne
- Stabilit√© sur diff√©rentes ex√©cutions

**Interpr√©tation** :
- Analyse des scores obtenus
- Comparaison entre algorithmes
- Choix du mod√®le final

---

## Profiling des groupes de clients

**Caract√©risation des clusters** :
- Profil d√©mographique (√¢ge, √©ducation, situation familiale)
- Comportement d'achat (montants, fr√©quence, canaux)
- R√©ponse aux campagnes marketing
- Pr√©f√©rences produits

**Insights business** :
- Segments de client√®le identifi√©s
- Strat√©gies de ciblage recommand√©es
- Opportunit√©s de cross-selling/up-selling
- Personnalisation des offres

**Visualisations** :
- Graphiques radar des profils
- Heatmaps des caract√©ristiques
- Analyse des correspondances
- Projections 2D/3D des clusters

---

## Conclusion

### Synth√®se du travail r√©alis√©
- √âtude th√©orique des algorithmes de clustering
- Impl√©mentation pratique d'une classe K-means
- Application sur datasets r√©els (Iris et Customer Personality)
- Comparaison et √©valuation des diff√©rents algorithmes

### Apprentissages cl√©s
- Importance du preprocessing des donn√©es
- Impact des param√®tres sur les r√©sultats
- N√©cessit√© d'utiliser plusieurs m√©triques d'√©valuation
- Compl√©mentarit√© des diff√©rents algorithmes

### Perspectives d'am√©lioration
- Test d'autres algorithmes (GMM, Mean Shift)
- Optimisation des hyperparam√®tres
- Validation temporelle des segments
- Int√©gration de donn√©es externes

---

## üìÅ Structure du projet

```
customer-personality-analysis/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ raw/
‚îÇ       ‚îî‚îÄ‚îÄ marketing_campaign.csv    # Dataset principal de marketing
‚îú‚îÄ‚îÄ exploration.ipynb                 # Analyse exploratoire des donn√©es
‚îú‚îÄ‚îÄ modelisation.ipynb                # Application des algorithmes de clustering
‚îú‚îÄ‚îÄ kmeans.py                         # Impl√©mentation classe K-means personnalis√©e
‚îî‚îÄ‚îÄ README.md                         # Documentation du projet
```

## üöÄ Comment utiliser ce projet

### Pr√©requis
```bash
pip install numpy pandas scikit-learn matplotlib seaborn jupyter
```

### √âtapes d'ex√©cution
1. **Exploration des donn√©es** : Lancez `exploration.ipynb` pour l'analyse exploratoire
2. **Test sur Iris** : Utilisez `kmeans.py` pour tester l'impl√©mentation K-means sur Iris
3. **Mod√©lisation** : Ex√©cutez `modelisation.ipynb` pour l'application des algorithmes de clustering
4. **Analyse** : Interpr√©tez les r√©sultats et profilez les segments clients

### Notebooks
- `exploration.ipynb` : Analyse exploratoire, preprocessing, r√©duction de dimension
- `modelisation.ipynb` : Application K-means, Hierarchical Clustering, DBSCAN et √©valuation

## üìö Ressources et r√©f√©rences

### Algorithmes de clustering
- [Scikit-learn Clustering](https://scikit-learn.org/stable/modules/clustering.html)
- [K-means Clustering](https://en.wikipedia.org/wiki/K-means_clustering)
- [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN)

### M√©triques d'√©valuation
- [Silhouette Analysis](https://scikit-learn.org/stable/modules/clustering.html#silhouette-analysis)
- [Calinski-Harabasz Index](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index)

### Dataset
- [Customer Personality Analysis](https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis)
- [Iris Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)